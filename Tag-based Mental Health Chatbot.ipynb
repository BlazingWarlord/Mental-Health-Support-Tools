{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrtz/YE0N6AOsa/MmOpwek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlazingWarlord/Mental-Health-Support-Tools/blob/main/Tag-based%20Mental%20Health%20Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5jlCxisWzLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow stopwords wordnet nltk"
      ],
      "metadata": {
        "id": "P3VPOhVALe8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfFJ3ESNLFEh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data_file = 'intents.json'\n",
        "\n",
        "with open(data_file) as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data():\n",
        "    words = []\n",
        "    classes = []\n",
        "    documents = []\n",
        "    ignore_words = ['?', '!']\n",
        "\n",
        "    for intent in data['intents']:\n",
        "        for pattern in intent['patterns']:\n",
        "            # Tokenize each word\n",
        "            word_list = nltk.word_tokenize(pattern)\n",
        "            words.extend(word_list)\n",
        "            documents.append((word_list, intent['tag']))\n",
        "            if intent['tag'] not in classes:\n",
        "                classes.append(intent['tag'])\n",
        "\n",
        "    # Lemmatize and lower each word and remove duplicates\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_words]\n",
        "    words = sorted(list(set(words)))\n",
        "    classes = sorted(list(set(classes)))\n",
        "\n",
        "    return words, classes, documents\n",
        "\n",
        "words, classes, documents = preprocess_data()\n"
      ]
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def prepare_training_data(words, classes, documents):\n",
        "    training = []\n",
        "    output_empty = [0] * len(classes)\n",
        "\n",
        "    for doc in documents:\n",
        "        bag = []\n",
        "        pattern_words = [lemmatizer.lemmatize(word.lower()) for word in doc[0]]\n",
        "        for word in words:\n",
        "            bag.append(1 if word in pattern_words else 0)\n",
        "\n",
        "        output_row = list(output_empty)\n",
        "        output_row[classes.index(doc[1])] = 1\n",
        "        training.append([bag, output_row])\n",
        "\n",
        "    # Print the lengths of bag and output_row for each document to check for inconsistencies\n",
        "    for item in training:\n",
        "        print(f\"Length of bag: {len(item[0])}, Length of output_row: {len(item[1])}\")\n",
        "\n",
        "    training = np.array(training, dtype=object) # Use dtype=object to handle inconsistent shapes\n",
        "    X = np.array([item[0] for item in training])\n",
        "    y = np.array([item[1] for item in training])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = prepare_training_data(words, classes, documents)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "a-RncyVTXdpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape=len(X[0]), output_shape=len(y[0]))\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wkZsqkZTWRhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "# Function to clean and preprocess user input\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Function to convert user input into a bag of words\n",
        "def bow(sentence, words):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "# Function to classify intent based on user input\n",
        "def classify_intent(sentence):\n",
        "    bow_vec = bow(sentence, words)\n",
        "    res = model.predict(np.array([bow_vec]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [(classes[r[0]], r[1]) for r in results]\n",
        "\n",
        "# Function to get the chatbot's response based on classified intent\n",
        "def get_response(intents_list, intents):\n",
        "    if intents_list:\n",
        "        tag = intents_list[0][0]\n",
        "        for intent in intents['intents']:\n",
        "            if intent['tag'] == tag:\n",
        "                return random.choice(intent['responses'])\n",
        "    return \"I didn't understand that.\"\n",
        "\n",
        "# Infinite chatbot loop\n",
        "while True:\n",
        "    message = input(\"You: \")\n",
        "    tag = classify_intent(message)\n",
        "    print(tag)\n",
        "    response = get_response(tag, data)\n",
        "    print(f\"Bot: {response}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g_euQnFZYbdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}